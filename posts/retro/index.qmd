---
title: "Thowing your data at chatGPT"
author: "Hamish Dickson"
date: "2023-03-26"
categories: [nlp, code]
draft: true
---

It is March 2023 and for nearly a month chatGPT has dominated every conversation I have about NLP. While it is an amazing tool (I paying for it now and I don't regret it) there are some limitations to RLHF only models. In this blog post I'm going to talk about them and then in my next post I'll go over the solution competitors are using.

## chatGPT

This isn't a chatGPT post, but to explain why it's not yet a golden hammer we need to understand how it's been trained and why that matters. Here is what we know about how it was trained:

- First, a very strong GPT model on a large corpus of text. Here we take a bunch of text and train the model to predict the next word (token) in the sequence. Not a lot more. To get a sentence the predicted word is added to the input and rerun. We don't know anything really about what chatGPT has been trained on, OpenAI have been very secretive about this. It's almost certainly a web crawl.
- a group of humans are asked to prompt and then rate the response positive or negative.
- this positive/negative labelling is used to build a policy model. As far as I can tell not much has been disclosed about this model, it's probably a BERT or something similar.
- Finally the policy model is used to train the GPT model to produce more positive responses using PPO.

## The problem

So here's the problem, chatGPT's "knowledge" comes from the initial GPT training and it relies on the model memorising key information. So it's totally unaware of anything after that point. This is actually one of the few things we know for certain about the dataset, it ends in September 2021, because the model actually tells us this.

