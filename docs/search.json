[
  {
    "objectID": "posts/hf_no_download/index.html",
    "href": "posts/hf_no_download/index.html",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "",
    "text": "Imagine you have been given the task of classifying product descriptions into categories. You have the data and compute to do what you need and because you are awesome at your job you do this in record time."
  },
  {
    "objectID": "posts/hf_no_download/index.html#the-simple-case",
    "href": "posts/hf_no_download/index.html#the-simple-case",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "The Simple Case",
    "text": "The Simple Case\nYou will probably be using Hugging Face’s Transformers library for this and you will probably start off with a simple model which looks like the following:\nimport transformers\n\nmodel_name = \"microsoft/deberta-v3-xsmall\"\nwhere_my_weights_live = \"my_save_location\"\n\n# create your tokenizer and model from the pretrained weights\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\n# let's pretend there are 10 labels\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)\n\n# ... train your model ...\n\n# finally save everything - this is all you need to do\ntokenizer.save_pretrained(where_my_weights_live)\nmodel.save_pretrained(where_my_weights_live)\nNow it’s time to put this in production and the engineering team asks for you help to make sure this is done properly. What does this look like?\nWell this is quite easy, you can basically just do this\nimport transformers\n\nwhere_my_weights_live = \"my_save_location\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(where_my_weights_live)\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(where_my_weights_live)\nIt’s easy to read, efficient and everyone is happy. Good job!"
  },
  {
    "objectID": "posts/hf_no_download/index.html#a-more-realistic-example",
    "href": "posts/hf_no_download/index.html#a-more-realistic-example",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "A More Realistic Example",
    "text": "A More Realistic Example\nIn reality while the default models are very good you are likely to want to modify them. For example, imagine we want to insert some product features into our model. We would have to declare our own model in this case.\nIt could look a little like this\nimport torch\nimport transformers\n\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_name, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        \n        self.model = transformers.AutoModel.from_pretrained(model_name)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\nYou would have to save this a little differently than the default models, it would look a bit like this\n# declare our models and train our code\n\nwhere_my_weights_live = \"my_save_location\"\n\ntokenizer.save_pretrained(where_my_weights_live)\n\ntorch.save(model.state_dict(), f\"{where_my_weights_live}/pytorch_model.bin\")\nYou can no longer just do model.save_pretrained instead we have to save the state_dict (ie the model weights).\nNow, how do you use this?\nWell, what’s the most obvious way to do this? It’s probably something like this?\nmodel_name = ...   # same as what we trained\nnum_features = ... # same as what we trained\nnum_labels = ...   # same as what we trained\n\n# pytorch doesn't save the model definition, so we have to declare it\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_name, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        \n        self.model = transformers.AutoModel.from_pretrained(model_name)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\n\n# now we can load the weights\nmodel = ProductClassifier(model_name, num_features, num_labels)\nmodel.load_state_dict(torch.load(where_my_weights_live, map_location=torch.device('cpu')))\nmodel.eval() # don't forget this!\nThis will probably work but something subtle happens — this will download the original model weights from Hugging Face!\nself.model = transformers.AutoModel.from_pretrained(model_name)\nThis line in our is the problematic line. It will download the weights from hugging face!\nThis isn’t something that’s going to break your code but it’s going to make it very slow to initialise and also you are downloading some weights that you will never actually use.\nInstead, can we initialise the model without downloading these weights?"
  },
  {
    "objectID": "posts/hf_no_download/index.html#using-from_config",
    "href": "posts/hf_no_download/index.html#using-from_config",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "Using from_config",
    "text": "Using from_config\nWe can do this\nmodel_name = ...   # same as what we trained\nnum_features = ... # same as what we trained\nnum_labels = ...   # same as what we trained\n\n# pytorch doesn't save the model definition, so we have to declare it\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_name, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        self.model_config = transformers.AutoConfig.from_pretrained(model_name)\n        self.model = transformers.AutoModel.from_config(config=self.model_config)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\n\n# now we can load the weights\nmodel = ProductClassifier(model_name, num_features, num_labels)\nmodel.load_state_dict(torch.load(where_my_weights_live, map_location=torch.device('cpu')))\nmodel.eval() # don't forget this!\nHere instead of using AutoModel.from_pretrained we are using AutoModel.from_config this will create a model from the config without downloading the pretrained weights, this is much better\nBut in this case we are still going to the internet and downloading the config. It would be better to do this without going to internet\nIdeally we would change our training setup so it looked more like this\nimport torch\nimport transformers\n\nmodel_name = ...\nnum_features = ...\nnum_labels = ...\n\nmodel_config = transformers.AutoConfig.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_config, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        self.model = transformers.AutoModel.from_config(config=model_config)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\n\n\nmodel = ProductClassifier(model_config, num_features, num_labels)\n\n# train the model\n\n# now you have to save one more thing, the model config\nmodel_config.save_pretrained(where_my_weights_live)\ntokenizer.save_pretrained(where_my_weights_live)\ntorch.save(model.state_dict(), f\"{where_my_weights_live}/pytorch_model.bin\")\nFinally our production code is just this!\nwhere_my_weights_live = ...\n\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(where_my_weights_live)\nmodel_config = transformers.AutoConfig.from_pretrained(where_my_weights_live)\n\n\nmodel = ProductClassifier(model_config, num_features, num_labels)\nmodel.load_state_dict(torch.load(where_my_weights_live, map_location=torch.device('cpu')))\nmodel.eval() # don't forget this!\nAnd now we can use our model without downloading anything!"
  },
  {
    "objectID": "posts/snowflake/index.html",
    "href": "posts/snowflake/index.html",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "",
    "text": "Snowflake has a great python connector called write_pandas which takes a pandas DataFrame and will use Arrow for type safety and clever compression techniques to upload it to Snowflake for you.\nBut sadly this is quite a large package and at a recent client using this wasn’t an option for various reasons. Unfortunitely I’ve always found the Snowflake docs… lacking or maybe just confusing on what your other options are.\nSo here’s what I found worked:"
  },
  {
    "objectID": "posts/snowflake/index.html#first-create-your-destination-table",
    "href": "posts/snowflake/index.html#first-create-your-destination-table",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "First create your destination table",
    "text": "First create your destination table\nCreate your table in Snowflake. There are many ways to do this, but lets just do it in a Worksheet.\ncreate or replace TABLE MY_DATABASE.MY_SCHEMA.MY_TABLE (\n    COL1 NUMBER(38,0) NOT NULL,\n    COL2 VARCHAR(20),\n    COL3 VARCHAR(200),\n);\nYou should see a helpful notifiction saying that this worked."
  },
  {
    "objectID": "posts/snowflake/index.html#now-create-your-file-format",
    "href": "posts/snowflake/index.html#now-create-your-file-format",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "Now create your file format",
    "text": "Now create your file format\nThis bit took me some time to figure out.\nYou need a file format. This seems to be a file telling Snowflake what to expect when data is uploaded with a PUT. I don’t know how I feel about the fact this exist, I don’t remember ever having to do anything like this for BigQuery but I can appreciate that the flexibility might be helpful to some.\nAgain, this is a Snowflake thing, do it in your Worksheet and you should see a success message\ncreate or replace file format csv_format\n  type = csv\n  field_delimiter = ','\n  skip_header = 1\n  null_if = ('NULL', 'null')\n  empty_field_as_null = true\n  compression = gzip\n  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n  ESCAPE = \"\\\\\"\n  ESCAPE_UNENCLOSED_FIELD = None"
  },
  {
    "objectID": "posts/snowflake/index.html#create-your-connection-to-snowflake",
    "href": "posts/snowflake/index.html#create-your-connection-to-snowflake",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "Create your connection to snowflake",
    "text": "Create your connection to snowflake\nFirst you will need a connection. You should be able to get most of this information from your admin\nimport snowflake.connector\n\nsnowflake_connection = snowflake.connector.connect(\n    user=SNOWFLAKE_USER,\n    password=SNOWFLAKE_PASSWORD,\n    account=SNOWFLAKE_ACCOUNT,\n    role=SNOWFLAKE_ROLE,\n    warehouse=SNOWFLAKE_WAREHOUSE,\n    database=\"MY_DATABASE\",\n    schema=\"MY_SCHEMA\",\n)\nOnce connected you only nee to run the following:\nsnowflake_connection.cursor().execute(f\"USE SCHEMA MY_SCHEMA\")\nsnowflake_connection.cursor().execute(f\"PUT file:///my_file.csv @%TEST_TABLE\")\nsnowflake_connection.cursor().execute(f\"COPY INTO TEST_TABLE file_format=(format_name = 'csv_format')\")\nThis will upload your file for you and append it to your TEST_TABLE. Note, I said append, this is a column orientated database designed for large datasets, so it will take you some work to do something more akin to UPSERT.\nHope that helped, contact me on Twitter if you have any questions."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A New Start",
    "section": "",
    "text": "Hi and thanks for getting this far!\nI’m excited to announce that after over 15 years of working in finance and then startups I have decided to become a freelancer!\nAs part of this I am (re)launching my blog. Most of my old posts have been converted to public notebooks on Kaggle, if you are interested please check them out.\nI hope here you will find interesting articles on what I’m working on, techniques I use a lot and hopefully a few demos. It will also act as a way for me to be more contactable… so please get in touch!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Using Hugging Face Transformer models without downloading the pretrained weights\n\n\n\n\n\n\n\nnlp\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2024\n\n\nHamish Dickson\n\n\n\n\n\n\n  \n\n\n\n\nLoading a CSV into Snowflake with Python\n\n\n\n\n\n\n\nshorts\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2023\n\n\nHamish Dickson\n\n\n\n\n\n\n  \n\n\n\n\nA New Start\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\nHamish Dickson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hamish Dickson",
    "section": "",
    "text": "Hi, Hamish here. I am a London (UK) based freelancer specialising in NLP and Deep Learning.\nI have spent over 15 years working in finance and multiple startups, building everything from SaaS solutions for some of the biggest investment banks in the world to B2C solutions in tiny startups.\nMy background is in Theoretical Physics which I studied at Imperial College London.\nIf you think I can help your business please get in touch, I am easily contactable via the links below!"
  }
]