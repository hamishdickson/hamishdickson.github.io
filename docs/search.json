[
  {
    "objectID": "posts/hf_no_download/index.html",
    "href": "posts/hf_no_download/index.html",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "",
    "text": "Imagine you have been given the task of classifying product descriptions into categories. You have the data and compute to do what you need and because you are awesome at your job you do this in record time."
  },
  {
    "objectID": "posts/hf_no_download/index.html#the-simple-case",
    "href": "posts/hf_no_download/index.html#the-simple-case",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "The Simple Case",
    "text": "The Simple Case\nYou will probably be using Hugging Face’s Transformers library for this and you will probably start off with a simple model which looks like the following:\nimport transformers\n\nmodel_name = \"microsoft/deberta-v3-xsmall\"\nwhere_my_weights_live = \"my_save_location\"\n\n# create your tokenizer and model from the pretrained weights\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\n# let's pretend there are 10 labels\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)\n\n# ... train your model ...\n\n# finally save everything - this is all you need to do\ntokenizer.save_pretrained(where_my_weights_live)\nmodel.save_pretrained(where_my_weights_live)\nNow it’s time to put this in production and the engineering team asks for you help to make sure this is done properly. What does this look like?\nWell this is quite easy, you can basically just do this\nimport transformers\n\nwhere_my_weights_live = \"my_save_location\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(where_my_weights_live)\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(where_my_weights_live)\nIt’s easy to read, efficient and everyone is happy. Good job!"
  },
  {
    "objectID": "posts/hf_no_download/index.html#a-more-realistic-example",
    "href": "posts/hf_no_download/index.html#a-more-realistic-example",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "A More Realistic Example",
    "text": "A More Realistic Example\nIn reality while the default models are very good you are likely to want to modify them. For example, imagine we want to insert some product features into our model. We would have to declare our own model in this case.\nIt could look a little like this\nimport torch\nimport transformers\n\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_name, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        \n        self.model = transformers.AutoModel.from_pretrained(model_name)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\nYou would have to save this a little differently than the default models, it would look a bit like this\n# declare our models and train our code\n\nwhere_my_weights_live = \"my_save_location\"\n\ntokenizer.save_pretrained(where_my_weights_live)\n\ntorch.save(model.state_dict(), f\"{where_my_weights_live}/pytorch_model.bin\")\nYou can no longer just do model.save_pretrained instead we have to save the state_dict (ie the model weights).\nNow, how do you use this?\nWell, what’s the most obvious way to do this? It’s probably something like this?\nmodel_name = ...   # same as what we trained\nnum_features = ... # same as what we trained\nnum_labels = ...   # same as what we trained\n\n# pytorch doesn't save the model definition, so we have to declare it\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_name, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        \n        self.model = transformers.AutoModel.from_pretrained(model_name)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\n\n# now we can load the weights\nmodel = ProductClassifier(model_name, num_features, num_labels)\nmodel.load_state_dict(torch.load(where_my_weights_live, map_location=torch.device('cpu')))\nmodel.eval() # don't forget this!\nThis will probably work but something subtle happens — this will download the original model weights from Hugging Face!\nself.model = transformers.AutoModel.from_pretrained(model_name)\nThis line in our is the problematic line. It will download the weights from hugging face!\nThis isn’t something that’s going to break your code but it’s going to make it very slow to initialise and also you are downloading some weights that you will never actually use.\nInstead, can we initialise the model without downloading these weights?"
  },
  {
    "objectID": "posts/hf_no_download/index.html#using-from_config",
    "href": "posts/hf_no_download/index.html#using-from_config",
    "title": "Using Hugging Face Transformer models without downloading the pretrained weights",
    "section": "Using from_config",
    "text": "Using from_config\nWe can do this\nmodel_name = ...   # same as what we trained\nnum_features = ... # same as what we trained\nnum_labels = ...   # same as what we trained\n\n# pytorch doesn't save the model definition, so we have to declare it\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_name, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        self.model_config = transformers.AutoConfig.from_pretrained(model_name)\n        self.model = transformers.AutoModel.from_config(config=self.model_config)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\n\n# now we can load the weights\nmodel = ProductClassifier(model_name, num_features, num_labels)\nmodel.load_state_dict(torch.load(where_my_weights_live, map_location=torch.device('cpu')))\nmodel.eval() # don't forget this!\nHere instead of using AutoModel.from_pretrained we are using AutoModel.from_config this will create a model from the config without downloading the pretrained weights, this is much better\nBut in this case we are still going to the internet and downloading the config. It would be better to do this without going to internet\nIdeally we would change our training setup so it looked more like this\nimport torch\nimport transformers\n\nmodel_name = ...\nnum_features = ...\nnum_labels = ...\n\nmodel_config = transformers.AutoConfig.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\nclass ProductClassifier(torch.nn.Module):\n    def __init__(self, model_config, num_features, num_labels):\n        super(ProductClassifier, self).__init__()\n        self.model = transformers.AutoModel.from_config(config=model_config)\n\n        self.fc_features = torch.nn.Linear(n_features, n_features)\n\n        model_hidden_size = self.model.config.hidden_size\n        self.fc_out = torch.nn.Linear(model_hidden_size + num_features, num_labels)\n\n    def forward(self, inputs, features):\n        model_outputs = self.model(**inputs)\n        model_outputs = model_outputs.last_hidden_state[:, 0, :]\n\n        feature_outputs = self.fc_features(features)\n\n        outputs = torch.cat([model_outputs, feature_outputs], dim=1)\n\n        return self.fc_out(outputs)\n\n\nmodel = ProductClassifier(model_config, num_features, num_labels)\n\n# train the model\n\n# now you have to save one more thing, the model config\nmodel_config.save_pretrained(where_my_weights_live)\ntokenizer.save_pretrained(where_my_weights_live)\ntorch.save(model.state_dict(), f\"{where_my_weights_live}/pytorch_model.bin\")\nFinally our production code is just this!\nwhere_my_weights_live = ...\n\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(where_my_weights_live)\nmodel_config = transformers.AutoConfig.from_pretrained(where_my_weights_live)\n\n\nmodel = ProductClassifier(model_config, num_features, num_labels)\nmodel.load_state_dict(torch.load(where_my_weights_live, map_location=torch.device('cpu')))\nmodel.eval() # don't forget this!\nAnd now we can use our model without downloading anything!"
  },
  {
    "objectID": "posts/hf_grad_chk/index.html",
    "href": "posts/hf_grad_chk/index.html",
    "title": "How to use Gradient Checkpointing with Hugging Face models",
    "section": "",
    "text": "Imagine you are GPU poor and would like to train a huge model, how do you do it?\nWell your first approach is probably to use gradient accumulation, using multiple forward passes for each backward pass. When you average out the losses you can get very similar results to a bigger batch.\nBut … what if you can’t even do a single forward pass? You just don’t have enough GPU vRAM?\nGradient checkpointing is an easy way to get around this. Here is what you need to do, when you declare your model just add model.gradient_checkpointing_enable()\nimport transformers\n# note: you need to import this line, it's missing from almost all documentation\nimport torch.utils.checkpoint\n\nmodel = transformers.AutoModel.from_pretrained(\"my_huge_model\")\nmodel.gradient_checkpointing_enable()\nAnd that’s it, you get much more freedom to train your model.\nSo what is this doing? Well the short answer is we are trading speed for memory and we are doing it by moving some of our intermediate data off of our GPU and into memory.\nYou should play about with batch sizes here to try and find a balance between speed and batch size."
  },
  {
    "objectID": "posts/snowflake/index.html",
    "href": "posts/snowflake/index.html",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "",
    "text": "Snowflake has a great python connector called write_pandas which takes a pandas DataFrame and will use Arrow for type safety and clever compression techniques to upload it to Snowflake for you.\nBut sadly this is quite a large package and at a recent client using this wasn’t an option for various reasons. Unfortunitely I’ve always found the Snowflake docs… lacking or maybe just confusing on what your other options are.\nSo here’s what I found worked:"
  },
  {
    "objectID": "posts/snowflake/index.html#first-create-your-destination-table",
    "href": "posts/snowflake/index.html#first-create-your-destination-table",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "First create your destination table",
    "text": "First create your destination table\nCreate your table in Snowflake. There are many ways to do this, but lets just do it in a Worksheet.\ncreate or replace TABLE MY_DATABASE.MY_SCHEMA.MY_TABLE (\n    COL1 NUMBER(38,0) NOT NULL,\n    COL2 VARCHAR(20),\n    COL3 VARCHAR(200),\n);\nYou should see a helpful notifiction saying that this worked."
  },
  {
    "objectID": "posts/snowflake/index.html#now-create-your-file-format",
    "href": "posts/snowflake/index.html#now-create-your-file-format",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "Now create your file format",
    "text": "Now create your file format\nThis bit took me some time to figure out.\nYou need a file format. This seems to be a file telling Snowflake what to expect when data is uploaded with a PUT. I don’t know how I feel about the fact this exist, I don’t remember ever having to do anything like this for BigQuery but I can appreciate that the flexibility might be helpful to some.\nAgain, this is a Snowflake thing, do it in your Worksheet and you should see a success message\ncreate or replace file format csv_format\n  type = csv\n  field_delimiter = ','\n  skip_header = 1\n  null_if = ('NULL', 'null')\n  empty_field_as_null = true\n  compression = gzip\n  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n  ESCAPE = \"\\\\\"\n  ESCAPE_UNENCLOSED_FIELD = None"
  },
  {
    "objectID": "posts/snowflake/index.html#create-your-connection-to-snowflake",
    "href": "posts/snowflake/index.html#create-your-connection-to-snowflake",
    "title": "Loading a CSV into Snowflake with Python",
    "section": "Create your connection to snowflake",
    "text": "Create your connection to snowflake\nFirst you will need a connection. You should be able to get most of this information from your admin\nimport snowflake.connector\n\nsnowflake_connection = snowflake.connector.connect(\n    user=SNOWFLAKE_USER,\n    password=SNOWFLAKE_PASSWORD,\n    account=SNOWFLAKE_ACCOUNT,\n    role=SNOWFLAKE_ROLE,\n    warehouse=SNOWFLAKE_WAREHOUSE,\n    database=\"MY_DATABASE\",\n    schema=\"MY_SCHEMA\",\n)\nOnce connected you only nee to run the following:\nsnowflake_connection.cursor().execute(f\"USE SCHEMA MY_SCHEMA\")\nsnowflake_connection.cursor().execute(f\"PUT file:///my_file.csv @%TEST_TABLE\")\nsnowflake_connection.cursor().execute(f\"COPY INTO TEST_TABLE file_format=(format_name = 'csv_format')\")\nThis will upload your file for you and append it to your TEST_TABLE. Note, I said append, this is a column orientated database designed for large datasets, so it will take you some work to do something more akin to UPSERT.\nHope that helped, contact me on Twitter if you have any questions."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A New Start",
    "section": "",
    "text": "Hi and thanks for getting this far!\nI’m excited to announce that after over 15 years of working in finance and then startups I have decided to become a freelancer!\nAs part of this I am (re)launching my blog. Most of my old posts have been converted to public notebooks on Kaggle, if you are interested please check them out.\nI hope here you will find interesting articles on what I’m working on, techniques I use a lot and hopefully a few demos. It will also act as a way for me to be more contactable… so please get in touch!"
  },
  {
    "objectID": "posts/applications/index.html",
    "href": "posts/applications/index.html",
    "title": "Applications",
    "section": "",
    "text": "A comprehensive iOS app for calculating compound interest and visualizing savings growth over time.\n\n\nMulti-Currency Support - Supports 10 major currencies: USD, EUR, GBP, CAD, AUD, JPY, CHF, SEK, NOK, DKK - Automatic currency detection based on your device’s locale - Proper currency formatting with native symbols\nAdvanced Calculations - Initial deposit amount - Monthly contribution planning - Annual interest rate input (with percentage conversion) - Time horizon in years - Monthly compounding for accurate results\nVisual Analytics - Interactive line chart showing savings growth over time - Dual-line visualization: Total Value vs. Contributions - Year-by-year breakdown of your savings journey - Clear distinction between money contributed and interest earned\nUser-Friendly Interface - Clean, intuitive design with green accent colors - Input validation with real-time feedback - Results summary with highlighted future value - Responsive layout optimized for all iOS devices\n\n\n\n\nSet Your Initial Amount: Enter your starting savings balance\nChoose Currency: Select from supported currencies or use auto-detected currency\nPlan Monthly Contributions: Set how much you’ll save each month\nSet Interest Rate: Enter expected annual return (e.g., 4.0 for 4%)\nDefine Time Horizon: Choose how many years to project\nCalculate: Tap the calculate button to see your results\n\n\n\n\nFuture Value: Your total savings at the end of the time period Total Contributions: Sum of your initial deposit plus all monthly contributions Interest Earned: The difference between future value and total contributions\nThe chart shows two lines: - Green line (Total Value): Your complete savings including compound interest - Blue dashed line (Contributions): Just the money you’ve contributed over time\n\n\n\n\nBuilt with SwiftUI for modern iOS experience\nUses Charts framework for data visualization\nImplements proper compound interest formulas with monthly compounding\nSupports iOS 15.0 and later\nOptimized for iPhone and iPad\n\n\n\n\nThis app does not collect, store, or transmit any personal data. All calculations are performed locally on your device.\n\n\n\nFor questions, feedback, or technical support, please contact: [your-email@domain.com]\n\n\n\nVersion 1.0 (September 2025) - Initial release - Multi-currency support - Interactive savings calculator - Visual growth charts - Comprehensive results display"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Applications\n\n\n\n\n\n\n\napps\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\nHamish Dickson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Gradient Checkpointing with Hugging Face models\n\n\n\n\n\n\n\nnlp\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2024\n\n\nHamish Dickson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hugging Face Transformer models without downloading the pretrained weights\n\n\n\n\n\n\n\nnlp\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2024\n\n\nHamish Dickson\n\n\n\n\n\n\n  \n\n\n\n\nLoading a CSV into Snowflake with Python\n\n\n\n\n\n\n\nshorts\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2023\n\n\nHamish Dickson\n\n\n\n\n\n\n  \n\n\n\n\nA New Start\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\nHamish Dickson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hamish Dickson",
    "section": "",
    "text": "Hi, I’m Hamish Dickson, a Senior Machine Learning Engineer based in London, UK. I specialize in delivering production-ready ML solutions with a focus on NLP, LLMs, and deep learning systems.\nCurrently, I’m working as a Senior Machine Learning Engineer (Contract) at Spotify, where I’m exploring innovative approaches to ad optimization and recommendation systems. I enjoy tackling complex ML challenges and turning research into practical solutions that make a real impact."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "Hamish Dickson",
    "section": "Background",
    "text": "Background\nI have over 15 years of experience working across finance, startups, and tech companies. My journey spans from building SaaS solutions for major investment banks to developing ML systems at fast-growing startups like Sprout.ai and DriveTribe.\nMy academic background is in Theoretical Physics from Imperial College London, where I studied Quantum Field Theory, General Relativity, and Quantum Optics."
  },
  {
    "objectID": "about.html#technical-expertise",
    "href": "about.html#technical-expertise",
    "title": "Hamish Dickson",
    "section": "Technical Expertise",
    "text": "Technical Expertise\nI work primarily with Python, PyTorch, and Hugging Face Transformers, specializing in: - Large Language Models (LLMs) and fine-tuning - NLP and text processing systems - OCR and document understanding - Streaming systems and real-time ML - Reinforcement Learning\nI also have spent a great deal of my career working on streaming systems, specifically using Scala, Kafka, and Flink. More recently at Spotify I have been using scio."
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "Hamish Dickson",
    "section": "Get in touch",
    "text": "Get in touch\nIf you think I can help your business with ML solutions, please get in touch!"
  }
]